\Mysection{Introduction}

The field of time series forecasting has seen significant advancements with the development of deep learning models that can effectively capture complex temporal patterns. Among these innovations, the Temporal Fusion Transformer (TFT) represents a notable breakthrough in multi-horizon forecasting capabilities. Introduced by Lim et al.~\cite{Lim:2019}, TFT addresses the limitations of traditional forecasting models by effectively handling a complex mix of input types that are commonly encountered in real-world forecasting scenarios.

Multi-horizon forecasting problems frequently involve diverse inputs, including static (time-invariant) covariates, known future inputs, and exogenous time series that are only historically observed. While several deep learning models have been proposed for multi-step prediction, they typically consist of black-box models that do not account for the full range of inputs present in common scenarios~\cite{Lim:2019}. The TFT architecture was specifically designed to overcome these limitations.

What sets TFT apart from other forecasting models is its novel attention-based architecture that combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. This dual capability makes it particularly valuable for complex time series forecasting tasks where understanding the drivers behind predictions is as important as the accuracy of the forecasts themselves.

In practical applications, TFT has demonstrated impressive performance. During the VN1 forecasting competition, for instance, a TFT-based approach achieved 4th place overall and ranked 1st among non-ensemble models~\cite{AIHorizonForecast:2023}. This performance highlights the model's effectiveness in real-world forecasting tasks. Similarly, in an emergency department overcrowding prediction task, TFT achieved a Mean Absolute Percentage Error (MAPE) of 9.87\% and a Root Mean Squared Error (RMSE) of 178 people/day~\cite{Chang:2022}, further demonstrating its practical utility.

This paper provides a comprehensive overview of the Temporal Fusion Transformer, detailing its architecture, implementation approaches, and applications in various domains. We examine how TFT effectively processes three types of variables: static covariates (time-invariant), known future inputs, and observed historical time series~\cite{Lim:2019}. Additionally, we explore the interpretability features that allow for variable importance assessment, temporal attention visualization, and seasonal pattern detection, making TFT a valuable tool for both prediction and insight generation in time series analysis.